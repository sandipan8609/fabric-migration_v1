{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Step 2: Load Data from ADLS to Fabric Warehouse\n",
        "\n",
        "This notebook loads data from Azure Data Lake Storage Gen2 (ADLS) to Microsoft Fabric Warehouse.\n",
        "\n",
        "## Process Overview\n",
        "1. Configure connection parameters\n",
        "2. Connect to target Fabric Warehouse\n",
        "3. Setup external objects (credential, data source, file format)\n",
        "4. Discover extracted tables in ADLS\n",
        "5. Create schemas and tables in Fabric Warehouse\n",
        "6. Load data using COPY INTO\n",
        "7. Update statistics\n",
        "8. Validate row counts (optional)\n",
        "\n",
        "## Prerequisites\n",
        "- Completed Step 1: Data extraction to ADLS\n",
        "- Access to Microsoft Fabric Warehouse\n",
        "- Access to ADLS Gen2 storage account\n",
        "- Appropriate permissions (see PERMISSIONS_GUIDE.md)\n",
        "- ODBC Driver 17 for SQL Server installed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config-header",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Update the configuration parameters below with your environment details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target Fabric Warehouse Configuration\n",
        "target_workspace = \"<your-workspace-name>\"\n",
        "target_warehouse = \"<your-warehouse-name>\"\n",
        "\n",
        "# Azure Data Lake Storage Configuration\n",
        "storage_account = \"<your-storage-account>\"\n",
        "container = \"migration-staging\"\n",
        "\n",
        "# Optional: Source database for validation\n",
        "validate_row_counts = False  # Set to True to validate row counts against source\n",
        "source_server = \"<your-synapse-server>.sql.azuresynapse.net\"  # Only needed if validate_row_counts = True\n",
        "source_database = \"<your-database-name>\"  # Only needed if validate_row_counts = True\n",
        "\n",
        "# Migration Settings\n",
        "update_statistics = True  # Update statistics after loading\n",
        "batch_size = 50  # Number of tables to process in each batch\n",
        "\n",
        "# Authentication Configuration\n",
        "# Options: 'token', 'interactive'\n",
        "auth_type = 'token'  # Use 'token' in Fabric notebooks with managed identity\n",
        "\n",
        "print(\"Configuration loaded successfully ✓\")\n",
        "print(f\"Target: {target_workspace}/{target_warehouse}\")\n",
        "print(f\"Storage: {storage_account}/{container}\")\n",
        "print(f\"Row count validation: {'Enabled' if validate_row_counts else 'Disabled'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-header",
      "metadata": {},
      "source": [
        "## Setup and Import Helper Functions\n",
        "\n",
        "Load the migration helper functions for database connections and utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import helper functions\n",
        "import sys\n",
        "sys.path.append('/lakehouse/default/Files/notebooks/utils')\n",
        "\n",
        "from migration_helpers import ConnectionHelper, MigrationUtils, StorageHelper, Colors\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Helper functions imported successfully ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "connect-header",
      "metadata": {},
      "source": [
        "## Connect to Fabric Warehouse\n",
        "\n",
        "Establish connection to Microsoft Fabric Warehouse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "connect",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get authentication token for Fabric\n",
        "auth_config = {}\n",
        "\n",
        "if auth_type == 'token':\n",
        "    # Get token from Fabric runtime\n",
        "    token = ConnectionHelper.get_spark_token(\"https://analysis.windows.net/powerbi/api\")\n",
        "    auth_config = {'auth_type': 'token', 'token': token}\n",
        "else:\n",
        "    auth_config = {'auth_type': auth_type}\n",
        "\n",
        "# Connect to Fabric Warehouse\n",
        "target_conn = ConnectionHelper.connect_fabric_warehouse(target_workspace, target_warehouse, auth_config)\n",
        "\n",
        "# Optionally connect to source for validation\n",
        "source_conn = None\n",
        "if validate_row_counts:\n",
        "    print(\"\\nConnecting to source database for validation...\")\n",
        "    source_auth_config = {'auth_type': auth_type}\n",
        "    if auth_type == 'token':\n",
        "        source_token = ConnectionHelper.get_spark_token(\"https://database.windows.net/.default\")\n",
        "        source_auth_config['token'] = source_token\n",
        "    source_conn = ConnectionHelper.connect_azure_sql(source_server, source_database, source_auth_config)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Connections established successfully!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "external-objects-header",
      "metadata": {},
      "source": [
        "## Setup External Objects\n",
        "\n",
        "Create external objects in Fabric Warehouse required for data loading:\n",
        "- Database scoped credential\n",
        "- External data source pointing to ADLS\n",
        "- External file format (Parquet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "external-objects",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup external objects in Fabric Warehouse\n",
        "setup_success = MigrationUtils.setup_external_objects(target_conn, storage_account, container)\n",
        "\n",
        "if not setup_success:\n",
        "    raise Exception(\"Failed to setup external objects. Please check permissions and retry.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"External objects created successfully!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "discover-header",
      "metadata": {},
      "source": [
        "## Discover Tables in Storage\n",
        "\n",
        "Scan ADLS to identify all tables that were extracted and are ready to be loaded.\n",
        "\n",
        "This uses the folder structure (schema/table) to identify tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "discover",
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all schemas and tables from ADLS by scanning folder structure\n",
        "print(f\"{Colors.BLUE}Discovering tables in storage...{Colors.END}\")\n",
        "\n",
        "# Use Spark to list directories\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "storage_path = StorageHelper.get_adls_path(storage_account, container)\n",
        "\n",
        "# Get list of schemas (top-level directories)\n",
        "try:\n",
        "    # List all paths in the container\n",
        "    from pyspark.dbutils import DBUtils\n",
        "    dbutils = DBUtils(spark)\n",
        "    \n",
        "    schemas_list = dbutils.fs.ls(storage_path)\n",
        "    \n",
        "    tables_to_load = []\n",
        "    \n",
        "    for schema_item in schemas_list:\n",
        "        if schema_item.isDir():\n",
        "            schema_name = schema_item.name.rstrip('/')\n",
        "            \n",
        "            # List tables in this schema\n",
        "            tables_list = dbutils.fs.ls(schema_item.path)\n",
        "            \n",
        "            for table_item in tables_list:\n",
        "                if table_item.isDir():\n",
        "                    table_name = table_item.name.rstrip('/')\n",
        "                    tables_to_load.append((schema_name, table_name))\n",
        "    \n",
        "    print(f\"{Colors.GREEN}✅ Found {len(tables_to_load)} tables in storage{Colors.END}\\n\")\n",
        "    \n",
        "    # Display tables\n",
        "    if len(tables_to_load) > 0:\n",
        "        print(f\"{Colors.BOLD}Tables to load:{Colors.END}\")\n",
        "        for i, (schema, table) in enumerate(tables_to_load[:20], 1):\n",
        "            print(f\"  {i:2d}. [{schema}].[{table}]\")\n",
        "        if len(tables_to_load) > 20:\n",
        "            print(f\"  ... and {len(tables_to_load) - 20} more tables\")\n",
        "        print()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"{Colors.RED}❌ Failed to discover tables: {e}{Colors.END}\")\n",
        "    print(\"Note: Make sure you have access to the storage account and container.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-header",
      "metadata": {},
      "source": [
        "## Load Tables to Fabric Warehouse\n",
        "\n",
        "Load each table from ADLS to Fabric Warehouse using COPY INTO.\n",
        "\n",
        "This process:\n",
        "- Creates schemas if they don't exist\n",
        "- Creates tables with appropriate schema\n",
        "- Loads data using COPY INTO with retry logic\n",
        "- Tracks progress and errors\n",
        "- Optionally validates row counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tracking\n",
        "loading_stats = {\n",
        "    'total': len(tables_to_load),\n",
        "    'loaded': 0,\n",
        "    'failed': 0,\n",
        "    'validated': 0,\n",
        "    'validation_failed': 0,\n",
        "    'start_time': datetime.now()\n",
        "}\n",
        "\n",
        "failed_tables = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Starting Table Loading\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "cursor = target_conn.cursor()\n",
        "\n",
        "for idx, (schema, table) in enumerate(tables_to_load, 1):\n",
        "    try:\n",
        "        print(f\"\\n[{idx}/{len(tables_to_load)}] Loading [{schema}].[{table}]...\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Create schema if not exists\n",
        "        cursor.execute(f\"\"\"\n",
        "            IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = '{schema}')\n",
        "            BEGIN\n",
        "                EXEC('CREATE SCHEMA [{schema}]')\n",
        "            END\n",
        "        \"\"\")\n",
        "        \n",
        "        # Drop table if exists (for clean reload)\n",
        "        cursor.execute(f\"\"\"\n",
        "            IF EXISTS (SELECT * FROM sys.tables t\n",
        "                      INNER JOIN sys.schemas s ON t.schema_id = s.schema_id\n",
        "                      WHERE s.name = '{schema}' AND t.name = '{table}')\n",
        "            BEGIN\n",
        "                DROP TABLE [{schema}].[{table}]\n",
        "            END\n",
        "        \"\"\")\n",
        "        \n",
        "        # Load data using COPY INTO\n",
        "        location = f\"{schema}/{table}/\"\n",
        "        \n",
        "        # Retry logic\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                cursor.execute(f\"\"\"\n",
        "                    COPY INTO [{schema}].[{table}]\n",
        "                    FROM '{location}'\n",
        "                    WITH (\n",
        "                        DATA_SOURCE = 'MigrationStaging',\n",
        "                        FILE_TYPE = 'PARQUET',\n",
        "                        MAXERRORS = 10000,\n",
        "                        ERRORFILE = 'errors/{schema}/{table}/'\n",
        "                    )\n",
        "                \"\"\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = 2 ** attempt\n",
        "                    print(f\"   Retry {attempt + 1}/{max_retries} after {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise e\n",
        "        \n",
        "        # Get row count\n",
        "        cursor.execute(f\"SELECT COUNT(*) FROM [{schema}].[{table}]\")\n",
        "        row_count = cursor.fetchone()[0]\n",
        "        \n",
        "        target_conn.commit()\n",
        "        \n",
        "        duration = time.time() - start_time\n",
        "        loading_stats['loaded'] += 1\n",
        "        \n",
        "        print(f\"{Colors.GREEN}   ✅ Loaded {row_count:,} rows in {duration:.1f}s{Colors.END}\")\n",
        "        \n",
        "        # Validate row count if enabled\n",
        "        if validate_row_counts and source_conn:\n",
        "            validation = MigrationUtils.validate_row_count(source_conn, target_conn, schema, table)\n",
        "            if validation['status'] == 'success' and validation['match']:\n",
        "                loading_stats['validated'] += 1\n",
        "                print(f\"   {Colors.GREEN}✓ Row count validated{Colors.END}\")\n",
        "            elif validation['status'] == 'mismatch':\n",
        "                loading_stats['validation_failed'] += 1\n",
        "                print(f\"   {Colors.RED}✗ Row count mismatch: source={validation['source_count']:,}, target={validation['target_count']:,}{Colors.END}\")\n",
        "        \n",
        "        print(f\"   Progress: {loading_stats['loaded']}/{loading_stats['total']}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        loading_stats['failed'] += 1\n",
        "        failed_tables.append((schema, table, str(e)))\n",
        "        print(f\"{Colors.RED}   ❌ Failed: {e}{Colors.END}\")\n",
        "        continue\n",
        "\n",
        "loading_stats['end_time'] = datetime.now()\n",
        "\n",
        "# Print summary\n",
        "duration = (loading_stats['end_time'] - loading_stats['start_time']).total_seconds()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total tables:       {loading_stats['total']}\")\n",
        "print(f\"Loaded:             {Colors.GREEN}{loading_stats['loaded']}{Colors.END}\")\n",
        "print(f\"Failed:             {Colors.RED}{loading_stats['failed']}{Colors.END}\")\n",
        "\n",
        "if validate_row_counts:\n",
        "    print(f\"Validated:          {Colors.GREEN}{loading_stats['validated']}{Colors.END}\")\n",
        "    print(f\"Validation failed:  {Colors.RED}{loading_stats['validation_failed']}{Colors.END}\")\n",
        "\n",
        "print(f\"Duration:           {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
        "print(f\"Start time:         {loading_stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"End time:           {loading_stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if loading_stats['failed'] > 0:\n",
        "    print(f\"\\n{Colors.YELLOW}⚠️  Failed Tables:{Colors.END}\")\n",
        "    for schema, table, error in failed_tables:\n",
        "        print(f\"  - [{schema}].[{table}]: {error}\")\n",
        "else:\n",
        "    print(f\"\\n{Colors.GREEN}✅ All tables loaded successfully!{Colors.END}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "statistics-header",
      "metadata": {},
      "source": [
        "## Update Statistics\n",
        "\n",
        "Update statistics on all loaded tables for optimal query performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "statistics",
      "metadata": {},
      "outputs": [],
      "source": [
        "if update_statistics:\n",
        "    print(f\"{Colors.BLUE}Updating statistics...{Colors.END}\\n\")\n",
        "    \n",
        "    cursor = target_conn.cursor()\n",
        "    \n",
        "    # Get all user tables\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT s.name, t.name\n",
        "        FROM sys.tables t\n",
        "        INNER JOIN sys.schemas s ON t.schema_id = s.schema_id\n",
        "        WHERE s.name NOT IN ('sys', 'INFORMATION_SCHEMA')\n",
        "    \"\"\")\n",
        "    \n",
        "    all_tables = cursor.fetchall()\n",
        "    \n",
        "    stats_updated = 0\n",
        "    stats_failed = 0\n",
        "    \n",
        "    for schema, table in all_tables:\n",
        "        try:\n",
        "            cursor.execute(f\"UPDATE STATISTICS [{schema}].[{table}]\")\n",
        "            stats_updated += 1\n",
        "            print(f\"   ✓ [{schema}].[{table}]\")\n",
        "        except Exception as e:\n",
        "            stats_failed += 1\n",
        "            print(f\"   {Colors.YELLOW}⚠️  Failed for [{schema}].[{table}]: {e}{Colors.END}\")\n",
        "    \n",
        "    target_conn.commit()\n",
        "    print(f\"\\n{Colors.GREEN}✅ Statistics updated for {stats_updated} tables{Colors.END}\")\n",
        "    if stats_failed > 0:\n",
        "        print(f\"{Colors.YELLOW}⚠️  Failed to update statistics for {stats_failed} tables{Colors.END}\")\n",
        "else:\n",
        "    print(\"Statistics update skipped (update_statistics = False)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup-header",
      "metadata": {},
      "source": [
        "## Cleanup and Close Connections\n",
        "\n",
        "Close all database connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close connections\n",
        "if target_conn:\n",
        "    target_conn.close()\n",
        "    print(f\"{Colors.GREEN}✅ Target connection closed{Colors.END}\")\n",
        "\n",
        "if source_conn:\n",
        "    source_conn.close()\n",
        "    print(f\"{Colors.GREEN}✅ Source connection closed{Colors.END}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Loading process completed!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Verify tables in Fabric Warehouse\")\n",
        "print(\"2. Run notebook '03_validate_migration.ipynb' for comprehensive validation\")\n",
        "print(\"3. Test queries and performance\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
