{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Step 1: Extract Data from Azure Synapse to ADLS\n",
        "\n",
        "This notebook extracts data from Azure Synapse Dedicated SQL Pool to Azure Data Lake Storage Gen2 (ADLS).\n",
        "\n",
        "## Process Overview\n",
        "1. Configure connection parameters\n",
        "2. Connect to source database\n",
        "3. Setup external objects (credential, data source, file format)\n",
        "4. Discover tables to migrate\n",
        "5. Extract tables to ADLS in Parquet format\n",
        "6. Validate extraction results\n",
        "\n",
        "## Prerequisites\n",
        "- Access to Azure Synapse Dedicated SQL Pool\n",
        "- Access to ADLS Gen2 storage account\n",
        "- Appropriate permissions (see PERMISSIONS_GUIDE.md)\n",
        "- ODBC Driver 17 for SQL Server installed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config-header",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Update the configuration parameters below with your environment details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source Azure Synapse Configuration\n",
        "source_server = \"<your-synapse-server>.sql.azuresynapse.net\"\n",
        "source_database = \"<your-database-name>\"\n",
        "\n",
        "# Azure Data Lake Storage Configuration\n",
        "storage_account = \"<your-storage-account>\"\n",
        "container = \"migration-staging\"\n",
        "\n",
        "# Migration Settings\n",
        "enable_partitioning = True  # Enable partitioning for large tables (> 1 GB)\n",
        "batch_size = 50  # Number of tables to process in each batch\n",
        "\n",
        "# Authentication Configuration\n",
        "# Options: 'token', 'interactive', 'sql'\n",
        "auth_type = 'interactive'  # Use 'token' in Fabric notebooks with managed identity\n",
        "\n",
        "print(\"Configuration loaded successfully ✓\")\n",
        "print(f\"Source: {source_server}/{source_database}\")\n",
        "print(f\"Storage: {storage_account}/{container}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-header",
      "metadata": {},
      "source": [
        "## Setup and Import Helper Functions\n",
        "\n",
        "Load the migration helper functions for database connections and utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import helper functions\n",
        "import sys\n",
        "sys.path.append('/lakehouse/default/Files/notebooks/utils')\n",
        "\n",
        "from migration_helpers import ConnectionHelper, MigrationUtils, Colors\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Helper functions imported successfully ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "connect-header",
      "metadata": {},
      "source": [
        "## Connect to Source Database\n",
        "\n",
        "Establish connection to Azure Synapse Dedicated SQL Pool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "connect",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get authentication token if using Fabric notebook\n",
        "auth_config = {}\n",
        "\n",
        "if auth_type == 'token':\n",
        "    # Get token from Fabric runtime\n",
        "    token = ConnectionHelper.get_spark_token(\"https://database.windows.net/.default\")\n",
        "    auth_config = {'auth_type': 'token', 'token': token}\n",
        "else:\n",
        "    auth_config = {'auth_type': auth_type}\n",
        "\n",
        "# Connect to source database\n",
        "source_conn = ConnectionHelper.connect_azure_sql(source_server, source_database, auth_config)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Connection established successfully!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "external-objects-header",
      "metadata": {},
      "source": [
        "## Setup External Objects\n",
        "\n",
        "Create external objects required for data extraction:\n",
        "- Database scoped credential\n",
        "- External data source pointing to ADLS\n",
        "- External file format (Parquet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "external-objects",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup external objects\n",
        "setup_success = MigrationUtils.setup_external_objects(source_conn, storage_account, container)\n",
        "\n",
        "if not setup_success:\n",
        "    raise Exception(\"Failed to setup external objects. Please check permissions and retry.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"External objects created successfully!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "discover-header",
      "metadata": {},
      "source": [
        "## Discover Tables to Extract\n",
        "\n",
        "Scan the source database to identify all tables that need to be migrated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "discover",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get list of tables\n",
        "tables = MigrationUtils.get_tables_list(source_conn)\n",
        "\n",
        "if len(tables) == 0:\n",
        "    print(f\"{Colors.YELLOW}⚠️  No tables found to extract{Colors.END}\")\n",
        "else:\n",
        "    print(f\"\\n{Colors.GREEN}✅ Ready to extract {len(tables)} tables{Colors.END}\")\n",
        "    \n",
        "    # Store tables for reference\n",
        "    table_list = [(schema, table, rows, size) for schema, table, rows, size in tables]\n",
        "    \n",
        "    print(f\"\\nTotal data volume: {sum(t[3] for t in tables):.2f} GB\")\n",
        "    print(f\"Total rows: {sum(t[2] for t in tables):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "extract-header",
      "metadata": {},
      "source": [
        "## Extract Tables to ADLS\n",
        "\n",
        "Extract each table to ADLS Gen2 using CREATE EXTERNAL TABLE AS SELECT (CETAS).\n",
        "\n",
        "This process:\n",
        "- Creates external tables pointing to ADLS\n",
        "- Exports data in Parquet format with Snappy compression\n",
        "- Organizes data by schema/table folder structure\n",
        "- Tracks progress and errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extract",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tracking\n",
        "extraction_stats = {\n",
        "    'total': len(tables),\n",
        "    'extracted': 0,\n",
        "    'failed': 0,\n",
        "    'start_time': datetime.now()\n",
        "}\n",
        "\n",
        "failed_tables = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Starting Table Extraction\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "cursor = source_conn.cursor()\n",
        "\n",
        "for idx, (schema, table, row_count, size_gb) in enumerate(tables, 1):\n",
        "    try:\n",
        "        print(f\"\\n[{idx}/{len(tables)}] Extracting [{schema}].[{table}]...\")\n",
        "        print(f\"   Rows: {row_count:,} | Size: {size_gb:.2f} GB\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Drop external table if exists\n",
        "        external_table_name = f\"ext_{table}_migration\"\n",
        "        cursor.execute(f\"\"\"\n",
        "            IF EXISTS (SELECT * FROM sys.external_tables WHERE name = '{external_table_name}')\n",
        "                DROP EXTERNAL TABLE [{schema}].[{external_table_name}]\n",
        "        \"\"\")\n",
        "        \n",
        "        # Create external table with CETAS\n",
        "        location = f\"{schema}/{table}/\"\n",
        "        \n",
        "        cursor.execute(f\"\"\"\n",
        "            CREATE EXTERNAL TABLE [{schema}].[{external_table_name}]\n",
        "            WITH (\n",
        "                LOCATION = '{location}',\n",
        "                DATA_SOURCE = MigrationStaging,\n",
        "                FILE_FORMAT = ParquetFormat\n",
        "            )\n",
        "            AS\n",
        "            SELECT * FROM [{schema}].[{table}]\n",
        "        \"\"\")\n",
        "        \n",
        "        source_conn.commit()\n",
        "        \n",
        "        duration = time.time() - start_time\n",
        "        extraction_stats['extracted'] += 1\n",
        "        \n",
        "        print(f\"{Colors.GREEN}   ✅ Completed in {duration:.1f}s{Colors.END}\")\n",
        "        print(f\"   Progress: {extraction_stats['extracted']}/{extraction_stats['total']}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        extraction_stats['failed'] += 1\n",
        "        failed_tables.append((schema, table, str(e)))\n",
        "        print(f\"{Colors.RED}   ❌ Failed: {e}{Colors.END}\")\n",
        "        continue\n",
        "\n",
        "extraction_stats['end_time'] = datetime.now()\n",
        "\n",
        "# Print summary\n",
        "duration = (extraction_stats['end_time'] - extraction_stats['start_time']).total_seconds()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXTRACTION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total tables:     {extraction_stats['total']}\")\n",
        "print(f\"Extracted:        {Colors.GREEN}{extraction_stats['extracted']}{Colors.END}\")\n",
        "print(f\"Failed:           {Colors.RED}{extraction_stats['failed']}{Colors.END}\")\n",
        "print(f\"Duration:         {duration:.1f} seconds ({duration/60:.1f} minutes)\")\n",
        "print(f\"Start time:       {extraction_stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"End time:         {extraction_stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if extraction_stats['failed'] > 0:\n",
        "    print(f\"\\n{Colors.YELLOW}⚠️  Failed Tables:{Colors.END}\")\n",
        "    for schema, table, error in failed_tables:\n",
        "        print(f\"  - [{schema}].[{table}]: {error}\")\n",
        "else:\n",
        "    print(f\"\\n{Colors.GREEN}✅ All tables extracted successfully!{Colors.END}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup-header",
      "metadata": {},
      "source": [
        "## Cleanup and Close Connection\n",
        "\n",
        "Clean up external tables and close database connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close connection\n",
        "if source_conn:\n",
        "    source_conn.close()\n",
        "    print(f\"{Colors.GREEN}✅ Connection closed{Colors.END}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Extraction process completed!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Verify data in ADLS: Check the container for extracted Parquet files\")\n",
        "print(\"2. Run notebook '02_load_data.ipynb' to load data into Fabric Warehouse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verification-header",
      "metadata": {},
      "source": [
        "## Verify Extraction (Optional)\n",
        "\n",
        "You can verify the extracted data by checking the ADLS container or using Azure Storage Explorer.\n",
        "\n",
        "Expected folder structure:\n",
        "```\n",
        "migration-staging/\n",
        "├── schema1/\n",
        "│   ├── table1/\n",
        "│   │   └── *.parquet\n",
        "│   └── table2/\n",
        "│       └── *.parquet\n",
        "└── schema2/\n",
        "    └── table3/\n",
        "        └── *.parquet\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
